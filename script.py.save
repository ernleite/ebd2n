import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from ignite.engine import Engine, Events
from ignite.distributed import DistributedProxySampler
import numpy as np
import os
import socket

def setup_distributed(rank, world_size, master_addr, master_port):
    """Initialize distributed training"""
    os.environ['MASTER_ADDR'] = master_addr
    os.environ['MASTER_PORT'] = str(master_port)
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def train_step(engine, batch):
    """Training step for Ignite engine"""
    # Get the data portion for this rank
    data = batch['data']
    rank = dist.get_rank()
    
    # Simple computation example
    result = torch.sum(data ** 2)
    
    print(f"Rank {rank} processed data shape: {data.shape}, result: {result.item()}")
    
    # Gather results from all processes
    gathered_results = [torch.zeros_like(result) for _ in range(dist.get_world_size())]
    dist.all_gather(gathered_results, result)
    
    if rank == 0:
        total_result = sum(r.item() for r in gathered_results)
        print(f"Total result across all ranks: {total_result}")
    
    return result

def main():
    # Configuration
    world_size = 2
    master_addr = "192.168.1.191"  # Pilot machine IP
    master_port = 12355
    
    # This is the worker (rank 1)
    rank = 1
    
    # Setup distributed training
    setup_distributed(rank, world_size, master_addr, master_port)
    
    # The worker will receive its data portion through distributed communication
    # In a real scenario, you'd implement proper data distribution
    # For now, we'll simulate receiving the second half of the matrix
    
    # Create a placeholder for the data this worker will receive
    # In the actual implementation, this would be sent from the pilot
    local_data = torch.randn(5, 10)  # Second half of the 10x10 matrix
    
    # Create Ignite engine
    trainer = Engine(train_step)
    
    # Add event handlers
    @trainer.on(Events.STARTED)
    def log_training_start(engine):
        print(f"Training started on rank {rank}")
    
    @trainer.on(Events.COMPLETED)
    def log_training_completion(engine):
        print(f"Training completed on rank {rank}")
    
    # Create data loader (simple example)
    data_loader = [{'data': local_data} for _ in range(5)]  # 5 iterations
    
    # Run training
    trainer.run(data_loader)
    
    # Cleanup
    dist.destroy_process_group()

if __name__ == "__main__":
    main()
